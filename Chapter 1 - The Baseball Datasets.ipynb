{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import get_data as gd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from os import walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to the Data Sets\n",
    "\n",
    "This notebook will introduce you to some foundational datasets for sabermetrics. An brief overview of the datasets, code to get the data, and some examples of how you can use the dataset to answer baseball questions will be provided.\n",
    "\n",
    "The datasets introduced will be:\n",
    "* The Lahman Dataset - It provides a large amount of season data from 1871 to the current season. It has an impressive amount of documentation and can be downloaded in a number of different sources (I use the csv version in this notebook)\n",
    "* The Retro Dataset - Provides game by game or play by play data, a different zip file exists for each season. The data can be found here www.retrosheet.org/gamelogs/index.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Lahman Dataset\n",
    "\n",
    "The section will go over (1) How to get the data to your local (2) Examples of questions you can answer with the data (3) Limitation of the dataset.\n",
    "\n",
    "### Getting the Data\n",
    "Neither function is to complex but both help save some time and manual steps. \n",
    "\n",
    "The first function \"get_data\" checks to see if a \"master\" folder already exists, if it doesn't it then uses the \"dload\" package to download and unzip all the files. Some potential good extensions to the get_data function would be to allow the user to provide the source link, name of the master file, and the option to update the content of the master file if it already exists.\n",
    "\n",
    "The \"make_df\" function just saves you from having to remeber the location of the lahman dataset.\n",
    "\n",
    "Again neither function is required, but makes life a litte bit easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \"\"\"\n",
    "    Function to download lahman data set, unzip and create a folder\n",
    "    \"\"\"\n",
    "    if path.exists('master'):\n",
    "        print('Folder already exists')\n",
    "    else:\n",
    "        dload.save_unzip(\"https://github.com/chadwickbureau/baseballdatabank/archive/master.zip\")\n",
    "\n",
    "\n",
    "def make_df(data_set: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Simple function that saves writting out the file path\n",
    "    :param data_set: Name of Lahman dataset you want\n",
    "    :return: A dataframe of the dataset\n",
    "    \"\"\"\n",
    "    assert isinstance(data_set, str)\n",
    "    df: pd.DataFrame = pd.read_csv(f'master/baseballdatabank-master/core/{data_set}.csv')\n",
    "    assert isinstance(df, pd.DataFrame)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Questions & Limitation\n",
    "\n",
    "Below are some example questions that will be answered, the Lahman dataset provides you with almost everything you need to answer season level questions. However, since all data is aggregated to a season you won't be able to dig into game/event specific questions such as \"Do hot streaks exists for batter or pitchers?\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1 - What is the average number of home runs per game recorded in each decade? Does the rate of strikeouts show any correlation with the rate of home runs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df = gd.make_df(\"Teams\") # Get the Teams data \n",
    "\n",
    "# Convert the year into decades\n",
    "teams_df['decade'] = round(teams_df['yearID']/10)*10 \n",
    "# Use the groupby function to aggregate totals for G, HR and SO by decade\n",
    "# I tend to find to easier to set as_index to false\n",
    "decade_group = teams_df.groupby('decade', as_index=False)[['G','HR','SO']].sum()\n",
    "#This removes double counting games\n",
    "decade_group['G'] = decade_group['G']/2 \n",
    "# Now you just need to calculate the averages by decade :)\n",
    "decade_group['Avg HR Per Game'] = decade_group['HR']/decade_group['G']\n",
    "decade_group['Avg SO Per Game'] = decade_group['SO']/decade_group['G']\n",
    "\n",
    "# I will be using matplotlib, some other good libraries to check out are seaborn, plotly & altair\n",
    "# As some future work I will re-make these graphs with the other libraries\n",
    "# Style wise, I typically start with 538, then make some modification to make it with a white background\n",
    "plt.style.use('fivethirtyeight')\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "# Since we want to compare to different metrics over time I use the gricspec function\n",
    "# Given the x-axis align and the y-axis don't I find it better to stack the graphs vertically rather than horizontally\n",
    "# If they are side by side I find ppl tend not to notice the different in axis's \n",
    "# As a rule of thumb I try and always align axis of the same time, so in this case both graphs have x-axis that are by decade\n",
    "gs = fig.add_gridspec(2,1)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "fig.patch.set_facecolor('white')\n",
    "ax1.set_facecolor('white')\n",
    "ax1.plot(decade_group['decade'], decade_group['Avg HR Per Game'])\n",
    "ax1.set_title('Average Home Runs Per Game')\n",
    "\n",
    "ax2.set_facecolor('white')\n",
    "ax2.plot(decade_group['decade'], decade_group['Avg SO Per Game'])\n",
    "ax2.set_title('Average Strike Outs Per Game')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 - What effect has the introduction of the Designated Hitter (DH) in the American League had in the difference in run scoring between the American and National Leagues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similiar to above you can do some simple groupping by league & year for games and runs\n",
    "league_group = teams_df.groupby(['yearID','lgID'], as_index=False)[['G','R']].sum()\n",
    "\n",
    "# I then seperate out NL & AL into their own dataframes to make graphing & calculations easier \n",
    "nl_group = league_group[league_group['lgID'] == 'NL']\n",
    "al_group = league_group[league_group['lgID'] == 'AL']\n",
    "\n",
    "# Find the average runs per game (since we seperated leagues no need to remove double counting)\n",
    "nl_group['Avg Runs Per Game'] = nl_group['R']/nl_group['G']\n",
    "al_group['Avg Runs Per Game'] = al_group['R']/al_group['G']\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "# First attempt at graphing this I just plotted the two leagues over top of each other\n",
    "# This seemed like the natural way to look at it but it was hard to tell what the impact was\n",
    "fig, ax = plt.subplots(figsize= (20,10))\n",
    "fig.patch.set_facecolor('white')\n",
    "ax.plot(nl_group['yearID'], nl_group['Avg Runs Per Game'], label='National League')\n",
    "ax.plot(al_group['yearID'], al_group['Avg Runs Per Game'], label='American League')\n",
    "ax.set_facecolor('white')\n",
    "ax.legend()\n",
    "ax.set_title('Average Runs by League')\n",
    "# draw a line for when the DH was added to make seeing the impact easier\n",
    "# TODO Add some text indicating what this line is\n",
    "plt.axvline(x=1973, color='black', linestyle='--') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In my second attempt I merge the AL & NL sets together and then graph the difference\n",
    "league_merge = pd.merge(nl_group, al_group, on='yearID',suffixes=('_nl', '_al'))\n",
    "league_merge['Avg Run Difference'] = league_merge['Avg Runs Per Game_nl'] - league_merge['Avg Runs Per Game_al']\n",
    "\n",
    "# The impact of the DH is more clear to see in this method\n",
    "# The interesting thing to notice is how slowly how the impact of the DH has disappeared\n",
    "# This makes me want to dig into what is causing the decline in the impact\n",
    "plt.style.use('fivethirtyeight')\n",
    "fig, ax = plt.subplots(figsize= (20,10))\n",
    "fig.patch.set_facecolor('white')\n",
    "ax.plot(league_merge['yearID'], league_merge['Avg Run Difference'])\n",
    "ax.set_facecolor('white')\n",
    "#ax.legend()\n",
    "plt.axvline(x=1973, color='black', linestyle='--')\n",
    "plt.axhline(y=0, color='black') # Add in a solid line to spot the difference a bit easier\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3 - How does the percentage of games completed by the starting pitcher from 2000 to 2010 compare to the percentage of games 100 years before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitching_df = gd.make_df(\"Pitching\") # Get the Pitching data \n",
    "\n",
    "# I slice the data into two different dataframes based on the years in question\n",
    "# Best to make a function which takes date ranges to do this\n",
    "pitching_df_2000_to_2010 = pitching_df[(pitching_df['yearID'] >= 2000) & (pitching_df['yearID'] <= 2010)]\n",
    "# Use the sum function you can easily figure out the % completed for the decades\n",
    "avg_comp_rate_2000_to_2010 = pitching_df_2000_to_2010['CG'].sum()/pitching_df_2000_to_2010['GS'].sum()\n",
    "pitching_df_1900_to_1910 = pitching_df[(pitching_df['yearID'] >= 1900) & (pitching_df['yearID'] <= 1910)]\n",
    "avg_comp_rate_1900_to_1910 = pitching_df_1900_to_1910['CG'].sum()/pitching_df_1900_to_1910['GS'].sum()\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "fig, ax = plt.subplots(figsize= (20,10))\n",
    "fig.patch.set_facecolor('white')\n",
    "ax.bar(x = ['1900 to 1910', '2000 to 2010'],\n",
    "      height=[avg_comp_rate_1900_to_1910*100, avg_comp_rate_2000_to_2010*100])\n",
    "ax.set_facecolor('white')\n",
    "ax.yaxis.set_major_formatter(PercentFormatter())\n",
    "ax.set_ylim(0,100)\n",
    "ax.set_title('The Average Pitcher Game Completion Rate of\\n1900 to 1910 vs 2000 to 2010')\n",
    "plt.ylabel('% of Game Completed (Pitchers)')\n",
    "#ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Retro Dataset\n",
    "\n",
    "Like before this section will go over how to get the data and then some examples on what questions you can answer with the dataset.\n",
    "\n",
    "### Getting the data\n",
    "\n",
    "The first function will create a retro folder and then iterate through each year, download and unzip the file.\n",
    "\n",
    "The second function will go through all the years and combined them into one single dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retro_data():\n",
    "    retro_path = './retro'\n",
    "    os.mkdir(retro_path)\n",
    "    for year in range(1871,2020):\n",
    "        dload.save_unzip(f'https://www.retrosheet.org/gamelogs/gl{year}.zip', extract_path=retro_path , delete_after=True)\n",
    "        \n",
    "def make_gamelog_df():\n",
    "    df = pd.read_csv('https://raw.githubusercontent.com/maxtoki/baseball_R/master/data/game_log_header.csv')\n",
    "    cols = list(df.columns)\n",
    "    for year in range(1871,2020):\n",
    "        df_temp = pd.read_csv(f'retro/GL{year}.TXT',names=cols)\n",
    "        df = df.append(df_temp, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gd.make_gamelog_df()\n",
    "\n",
    "df['HR'] = df['VisitorHR']+df['HomeHR']\n",
    "df['GameCounter'] = 1\n",
    "df['Month'] = df['Date'].apply(lambda x: str(x)[4:6])\n",
    "df = df[~df['HR'].isnull()]\n",
    "df_month = df.groupby('Month', as_index=False)[['HR','GameCounter']].apply(lambda x : x.astype(int).sum())\n",
    "df_month['Avg Home Runs by Month'] = df_month['HR']/df_month['GameCounter']\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "fig, ax = plt.subplots(figsize= (20,10))\n",
    "fig.patch.set_facecolor('white')\n",
    "ax.bar(x = df_month['Month'],\n",
    "      height=df_month['Avg Home Runs by Month'])\n",
    "ax.set_facecolor('white')\n",
    "ax.set_title('What Month has the Most Home Runs?')\n",
    "plt.ylabel('Average Home Runs Per Game by Month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "park_data = pd.read_csv('https://www.retrosheet.org/parkcode.txt')\n",
    "df['Year'] = df['Date'].apply(lambda x: str(x)[:4])\n",
    "df_post_2000 = df[df['Year'].astype(int) >= 2000]\n",
    "\n",
    "df_park = df_post_2000.groupby('ParkID', as_index=False)[['HR','GameCounter']].apply(lambda x : x.astype(int).sum())\n",
    "df_park['Avg Home Runs by Park'] = df_park['HR']/df_park['GameCounter']\n",
    "\n",
    "df_park_merged = pd.merge(df_park, park_data, left_on='ParkID', right_on='PARKID')\n",
    "df_park_merged.sort_values(by = 'Avg Home Runs by Park', inplace=True)\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "fig, ax = plt.subplots(figsize= (10,15))\n",
    "fig.patch.set_facecolor('white')\n",
    "ax.barh(y = df_park_merged['NAME'],\n",
    "      width=df_park_merged['Avg Home Runs by Park'])\n",
    "ax.set_facecolor('white')\n",
    "ax.set_title('What Ballpark has the Most Home Runs?')\n",
    "#plt.ylabel('Average Home Runs Per Game by Month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - Do runs happen more frequently when some umpires are be hind the plate? What is the difference between the most pitcher friendly and the most hitter-friendly umpires?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_2000['Runs'] = df_post_2000['VisitorRunsScored']+df_post_2000['HomeRunsScore']\n",
    "df_umpire_grouped = df_post_2000.groupby('UmpireHName', as_index=False)[['Runs', 'GameCounter']].apply(lambda x : x.astype(int).sum())\n",
    "df_umpire_grouped['Runs Per Game'] = df_umpire_grouped['Runs']/df_umpire_grouped['GameCounter']\n",
    "\n",
    "df_umpire_grouped = df_umpire_grouped[df_umpire_grouped['GameCounter'] > 400]\n",
    "df_umpire_grouped.sort_values(by='Runs Per Game', inplace=True)\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "fig, ax = plt.subplots(figsize= (10,15))\n",
    "fig.patch.set_facecolor('white')\n",
    "ax.barh(y = df_umpire_grouped['UmpireHName'],\n",
    "      width=df_umpire_grouped['Runs Per Game'])\n",
    "ax.set_facecolor('white')\n",
    "ax.set_title('Which umpire allows the most runs?')\n",
    "#plt.ylabel('Average Home Runs Per Game by Month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - How many extra people attend ballgames during the weekend? What’s the average attendance by day of the week?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day_of_week = df_post_2000.groupby('DayOfWeek',as_index=False)['Attendence'].mean()\n",
    "day_of_week = {\n",
    "    'Mon':1,\n",
    "    'Tue':2,\n",
    "    'Wed':3,\n",
    "    'Thu':4,\n",
    "    'Fri':5,\n",
    "    'Sat':6,\n",
    "    'Sun':7\n",
    "}\n",
    "df_day_of_week['DayOrdering'] = df_day_of_week['DayOfWeek'].map(day_of_week)\n",
    "df_day_of_week.sort_values(by='DayOrdering',inplace=True)\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "fig, ax = plt.subplots(figsize= (20,10))\n",
    "fig.patch.set_facecolor('white')\n",
    "ax.bar(x = df_day_of_week['DayOfWeek'],\n",
    "      height=df_day_of_week['Attendence'])\n",
    "ax.set_facecolor('white')\n",
    "ax.set_title('What Day of the Week has the Highest Attendence?')\n",
    "plt.ylabel('Average Attendence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dload\n",
    "import pandas as pd\n",
    "import os\n",
    "import get_data as gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd.get_event_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of passed values is 6, index implies 3.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-87e30bc0c4a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m'info'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0minfo_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mappend_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimary_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'start'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-87e30bc0c4a5>\u001b[0m in \u001b[0;36mappend_row\u001b[0;34m(append_row, df, pkey)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mappend_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappend_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mappend_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mrow_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappend_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_series\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m                         raise ValueError(\n\u001b[0m\u001b[1;32m    314\u001b[0m                             \u001b[0;34mf\"Length of passed values is {len(data)}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                             \u001b[0;34mf\"index implies {len(index)}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of passed values is 6, index implies 3."
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def append_row(append_row, df, pkey):\n",
    "    append_row[0]=pkey\n",
    "    row_series = pd.Series(append_row, index = df.columns)\n",
    "    df = df.append(row_series, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "files = []\n",
    "for (dirpath, dirnames, filenames) in walk('./game_event'):\n",
    "    files.extend(filenames)\n",
    "    \n",
    "info_df = pd.DataFrame(columns = ['id','one', 'two'])\n",
    "start_df = pd.DataFrame(columns = ['id','one', 'two', 'three', 'four', 'five'])\n",
    "play_df = pd.DataFrame(columns = ['id','one', 'two', 'three', 'four', 'five', 'siz'])\n",
    "com_df = pd.DataFrame(columns = ['id','one'])\n",
    "sub_df = pd.DataFrame(columns = ['id','one', 'two', 'three', 'four', 'five'])\n",
    "data_df = pd.DataFrame(columns = ['id','one', 'two', 'three'])\n",
    "    \n",
    "for file in files:\n",
    "    with open(f'./game_event/{file}') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "\n",
    "        for row in csv_reader:\n",
    "            if row[0] == 'id':\n",
    "                primary_key = row[1]\n",
    "\n",
    "            elif row[0] =='info':\n",
    "                info_df = append_row(row, info_df, primary_key)\n",
    "\n",
    "            elif row[0] == 'start':\n",
    "                start_df = append_row(row, start_df, primary_key)\n",
    "\n",
    "            elif row[0] == 'play':\n",
    "                play_df = append_row(row, play_df, primary_key)\n",
    "\n",
    "            elif row[0] == 'com':\n",
    "                com_df = append_row(row, com_df, primary_key)\n",
    "\n",
    "            elif row[0] == 'sub':\n",
    "                sub_df = append_row(row, sub_df, primary_key)\n",
    "\n",
    "            elif row[0] == 'data':\n",
    "                data_df = append_row(row, data_df, primary_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHN191604200', 'scorer', '?69', '109', '111', '485?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>one</th>\n",
       "      <th>two</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DET191604200</td>\n",
       "      <td>inputprogvers</td>\n",
       "      <td>version 7RS(19) of 07/07/92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DET191604200</td>\n",
       "      <td>visteam</td>\n",
       "      <td>CHA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DET191604200</td>\n",
       "      <td>hometeam</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DET191604200</td>\n",
       "      <td>date</td>\n",
       "      <td>1916/04/20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DET191604200</td>\n",
       "      <td>site</td>\n",
       "      <td>DET04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>CHN191604200</td>\n",
       "      <td>daynight</td>\n",
       "      <td>day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>CHN191604200</td>\n",
       "      <td>usedh</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>CHN191604200</td>\n",
       "      <td>translator</td>\n",
       "      <td>Cramer/Herdman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>CHN191604200</td>\n",
       "      <td>inputter</td>\n",
       "      <td>Dick Cramer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9093</th>\n",
       "      <td>CHN191604200</td>\n",
       "      <td>inputtime</td>\n",
       "      <td>2015/04/27 8:30AM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9094 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id            one                          two\n",
       "0     DET191604200  inputprogvers  version 7RS(19) of 07/07/92\n",
       "1     DET191604200        visteam                          CHA\n",
       "2     DET191604200       hometeam                          DET\n",
       "3     DET191604200           date                   1916/04/20\n",
       "4     DET191604200           site                        DET04\n",
       "...            ...            ...                          ...\n",
       "9089  CHN191604200       daynight                          day\n",
       "9090  CHN191604200          usedh                        false\n",
       "9091  CHN191604200     translator               Cramer/Herdman\n",
       "9092  CHN191604200       inputter                  Dick Cramer\n",
       "9093  CHN191604200      inputtime            2015/04/27 8:30AM\n",
       "\n",
       "[9094 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
